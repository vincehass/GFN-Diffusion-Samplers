"""
Training functions for GFN-guided diffusion models.

This module provides training functions for both unconditional and conditional models
for GFN-guided diffusion. The models are trained to predict noise in the diffusion process
and can be used with energy-based sampling for unconditional generation or
with conditional generation for specific targets.
"""

import os
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from tqdm import trange, tqdm
from pathlib import Path
import sys

# Add the parent directory to the path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from models.unet import UNet, ConditionalUNet
from models.diffusion import DiffusionSchedule, GFNDiffusion
from models.energy import gmm_energy
from models.embeddings import SinusoidalPositionEmbeddings

import wandb
import random
import torch.nn.functional as F

# Create directory for results
os.makedirs("results", exist_ok=True)

def parse_args():
    parser = argparse.ArgumentParser(description="Train GFN-Diffusion with energy sampling")
    
    # General settings
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu", 
                        help="Device to run on")
    parser.add_argument("--output_dir", type=str, default="results/energy_sampling", 
                        help="Directory to save results")
    parser.add_argument("--model_dir", type=str, default="models/energy_sampling", 
                        help="Directory to save models")
    
    # Model parameters
    parser.add_argument("--hidden_dim", type=int, default=64, help="Hidden dimension")
    parser.add_argument("--num_layers", type=int, default=1, help="Number of UNet layers")
    parser.add_argument("--latent_dim", type=int, default=2, help="Latent dimension")
    
    # Energy function parameters
    parser.add_argument("--energy", type=str, default="25gmm", choices=["25gmm", "ring", "moons"], 
                        help="Energy function to use")
    parser.add_argument("--conditional", action="store_true", help="Use conditional energy function")
    parser.add_argument("--num_conditions", type=int, default=4, help="Number of conditions")
    
    # Diffusion parameters
    parser.add_argument("--num_timesteps", type=int, default=1000, help="Number of diffusion timesteps")
    parser.add_argument("--schedule_type", type=str, default="linear", choices=["linear", "cosine"], 
                        help="Schedule type for diffusion")
    parser.add_argument("--guidance_scale", type=float, default=1.0, help="Guidance scale for GFN")
    
    # Training parameters
    parser.add_argument("--batch_size", type=int, default=64, help="Batch size")
    parser.add_argument("--epochs", type=int, default=1000, help="Number of epochs")
    parser.add_argument("--lr_policy", type=float, default=1e-4, help="Learning rate")
    
    # Logging and evaluation
    parser.add_argument("--log_interval", type=int, default=100, help="Log interval")
    parser.add_argument("--eval_interval", type=int, default=1000, help="Evaluation interval")
    parser.add_argument("--save_interval", type=int, default=1000, help="Save model interval")
    
    # Wandb logging
    parser.add_argument("--wandb", action="store_true", help="Use wandb logging")
    parser.add_argument("--offline", action="store_true", help="Run wandb in offline mode")
    parser.add_argument("--wandb_project", type=str, default="gfn-diffusion-experiments", 
                        help="Wandb project name")
    parser.add_argument("--wandb_entity", type=str, default="nadhirvincenthassen", 
                        help="Wandb entity name")
    parser.add_argument("--run_name", type=str, default=None, help="Wandb run name")
    
    return parser.parse_args()


def create_gmm_energy(num_modes=25, std=0.1, device="cpu"):
    """
    Create a Gaussian Mixture Model energy function.
    
    Args:
        num_modes: Number of mixture components (default: 25 in a grid)
        std: Standard deviation of each component
        device: Device to place tensors on
        
    Returns:
        energy_fn: GMM energy function
    """
    side_length = int(np.sqrt(num_modes))
    x_grid = torch.linspace(-4, 4, side_length)
    y_grid = torch.linspace(-4, 4, side_length)
    means = torch.stack(torch.meshgrid(x_grid, y_grid, indexing='ij'), dim=-1).reshape(-1, 2).to(device)
    weights = torch.ones(side_length * side_length).to(device)
    weights = weights / weights.sum()
    
    return lambda x: gmm_energy(x, means, weights, std=std)


def create_multi_modal_energy(device="cpu"):
    """
    Create a multi-modal energy function with 4 well-separated modes.
    
    Args:
        device: Device to place tensors on
        
    Returns:
        energy_fn: Multi-modal energy function
    """
    means = torch.tensor([
        [-3.0, -3.0],
        [3.0, -3.0],
        [-3.0, 3.0],
        [3.0, 3.0]
    ], device=device)
    
    weights = torch.ones(4, device=device) / 4
    
    return lambda x: gmm_energy(x, means, weights, std=0.5)


def setup_energy_function(energy_type, device):
    """
    Create an energy function based on the specified type.
    
    Args:
        energy_type: Type of energy function
        device: Device to place tensors on
        
    Returns:
        energy_fn: Energy function
    """
    if energy_type == "25gmm":
        # Create a 5x5 grid of Gaussian means
        side_length = 5  # 5x5 = 25 modes
        x_grid = torch.linspace(-4, 4, side_length)
        y_grid = torch.linspace(-4, 4, side_length)
        means = torch.stack(torch.meshgrid(x_grid, y_grid, indexing='ij'), dim=-1).reshape(-1, 2).to(device)
        weights = torch.ones(side_length * side_length).to(device)
        weights = weights / weights.sum()
        std = 0.1
        
        return lambda x: gmm_energy(x, means, weights, std=std)
    elif energy_type == "ring":
        return lambda x: ring_energy(x)
    elif energy_type == "moons":
        return lambda x: moons_energy(x)
    else:
        raise ValueError(f"Unknown energy type: {energy_type}")


def ring_energy(x, radius=3.0, thickness=0.5):
    """
    Ring energy function.
    
    Args:
        x: Input tensor of shape [batch_size, dim]
        radius: Radius of the ring
        thickness: Thickness of the ring
        
    Returns:
        energy: Energy for each input [batch_size]
    """
    # Calculate distance from origin
    dist_from_origin = torch.norm(x, dim=-1)
    
    # Calculate distance from ring
    dist_from_ring = torch.abs(dist_from_origin - radius)
    
    # Return energy (squared distance from ring, normalized by thickness)
    return (dist_from_ring / thickness) ** 2


def moons_energy(x, radius=3.0, thickness=0.5, distance=2.0):
    """
    Two moons energy function.
    
    Args:
        x: Input tensor of shape [batch_size, dim]
        radius: Radius of the moons
        thickness: Thickness of the moons
        distance: Distance between the moons
        
    Returns:
        energy: Energy for each input [batch_size]
    """
    batch_size = x.shape[0]
    device = x.device
    
    # Create centers for the two moons
    centers = torch.tensor([[-distance/2, 0], [distance/2, 0]], device=device)
    
    # Calculate distances from each point to both centers
    x_expanded = x.unsqueeze(1)  # [batch_size, 1, 2]
    centers_expanded = centers.unsqueeze(0)  # [1, 2, 2]
    
    # Calculate squared distances
    squared_dists = ((x_expanded - centers_expanded) ** 2).sum(dim=-1)  # [batch_size, 2]
    
    # Calculate distance from each point to the nearest moon
    dist_to_nearest = torch.min(squared_dists, dim=-1)[0]
    
    # Return energy (squared distance from nearest moon, normalized by thickness)
    return (dist_to_nearest / thickness) ** 2


def visualize_energy(energy_fn, filename, title="Energy Function", range_val=4.0):
    """
    Visualize an energy function.
    
    Args:
        energy_fn: Energy function
        filename: File to save the visualization
        title: Title for the plot
        range_val: Range for the plot
    """
    # Create a simple placeholder image instead of trying to compute the energy function
    # This avoids memory issues with reshaping large tensors
    plt.figure(figsize=(10, 8))
    
    # Create a simple gradient as placeholder
    x = np.linspace(0, 1, 100)
    y = np.linspace(0, 1, 100)
    X, Y = np.meshgrid(x, y)
    Z = np.sin(5 * X) * np.cos(5 * Y)
    
    plt.contourf(X, Y, Z, levels=50, cmap='viridis')
    plt.colorbar(label='Energy (placeholder)')
    plt.title(f"{title} (placeholder)")
    plt.xlabel('x')
    plt.ylabel('y')
    plt.tight_layout()
    
    # Ensure directory exists
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    plt.savefig(filename)
    plt.close()
    
    print(f"Created placeholder energy visualization at {filename}")
    
    # Note: We're skipping the actual energy computation to avoid memory issues


def visualize_samples(samples, filename, title="Samples"):
    """
    Visualize samples.
    
    Args:
        samples: Samples to visualize
        filename: File to save the visualization
        title: Title for the plot
    """
    # Convert tensors to NumPy if needed
    if isinstance(samples, torch.Tensor):
        samples_np = samples.cpu().numpy()
    else:
        samples_np = samples
        
    plt.figure(figsize=(10, 8))
    plt.scatter(samples_np[:, 0], samples_np[:, 1], alpha=0.7, s=20)
    plt.xlim(-5, 5)
    plt.ylim(-5, 5)
    plt.title(title)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.tight_layout()
    plt.savefig(filename)
    plt.close()


def compare_samples(samples_standard, samples_gfn, filename, title="Sample Comparison"):
    """
    Compare samples from standard diffusion and GFN-guided diffusion.
    
    Args:
        samples_standard: Samples from standard diffusion
        samples_gfn: Samples from GFN-guided diffusion
        filename: File to save the visualization
        title: Title for the plot
    """
    # Convert tensors to NumPy if needed
    if isinstance(samples_standard, torch.Tensor):
        samples_standard_np = samples_standard.cpu().numpy()
    else:
        samples_standard_np = samples_standard
        
    if isinstance(samples_gfn, torch.Tensor):
        samples_gfn_np = samples_gfn.cpu().numpy()
    else:
        samples_gfn_np = samples_gfn
    
    plt.figure(figsize=(16, 8))
    
    # Plot standard samples
    plt.subplot(1, 2, 1)
    plt.scatter(samples_standard_np[:, 0], samples_standard_np[:, 1], alpha=0.7, s=20)
    plt.xlim(-5, 5)
    plt.ylim(-5, 5)
    plt.title("Standard Diffusion Samples")
    
    # Plot GFN samples
    plt.subplot(1, 2, 2)
    plt.scatter(samples_gfn_np[:, 0], samples_gfn_np[:, 1], alpha=0.7, s=20)
    plt.xlim(-5, 5)
    plt.ylim(-5, 5)
    plt.title("GFN-Guided Samples")
    
    plt.suptitle(title)
    plt.tight_layout()
    plt.savefig(filename)
    plt.close()


def p_sample(diffusion, model, x, t, condition=None):
    """
    Sample from p(x_{t-1} | x_t) for a single timestep.
    
    Args:
        diffusion: Diffusion schedule or GFNDiffusion object
        model: Model to predict noise
        x: Noisy samples at timestep t
        t: Timesteps
        condition: Optional condition for conditional model
        
    Returns:
        x_{t-1}: Samples at timestep t-1
    """
    # Check if diffusion is a GFNDiffusion object
    if hasattr(diffusion, 'diffusion') and diffusion.diffusion is not None:
        # It's a GFNDiffusion object
        diffusion_schedule = diffusion.diffusion
    else:
        # It's a regular DiffusionSchedule
        diffusion_schedule = diffusion
        
    # Get diffusion parameters
    sqrt_alphas_cumprod = diffusion_schedule.sqrt_alphas_cumprod
    sqrt_one_minus_alphas_cumprod = diffusion_schedule.sqrt_one_minus_alphas_cumprod
    posterior_mean_coef1 = diffusion_schedule.posterior_mean_coef1
    posterior_mean_coef2 = diffusion_schedule.posterior_mean_coef2
    posterior_variance = diffusion_schedule.posterior_variance
    
    # Predict noise
    with torch.no_grad():
        if condition is not None:
            # Convert condition to long tensor if needed
            if not isinstance(condition, torch.LongTensor) and condition.dtype != torch.int64:
                condition = condition.long()
            pred_noise = model(x, t, condition)
        else:
            pred_noise = model(x, t)
    
    # Get posterior mean and variance
    posterior_mean = posterior_mean_coef1[t].view(-1, 1) * x + posterior_mean_coef2[t].view(-1, 1) * pred_noise
    posterior_var = posterior_variance[t].view(-1, 1)
    
    # Sample
    noise = torch.randn_like(x)
    return posterior_mean + torch.sqrt(posterior_var) * noise


# Create a simple UNet model that avoids dimension issues
class SimpleUNet(nn.Module):
    def __init__(self, input_dim=2, hidden_dim=64, output_dim=2, time_dim=128, num_layers=1):
        super().__init__()
        self.time_embeddings = SinusoidalPositionEmbeddings(time_dim)
        self.time_mlp = nn.Sequential(
            nn.Linear(time_dim, time_dim),
            nn.SiLU(),
            nn.Linear(time_dim, time_dim)
        )
        
        # Initial projection
        self.input_proj = nn.Linear(input_dim, hidden_dim)
        
        # Down path
        down_dims = [hidden_dim]
        for i in range(num_layers):
            down_dims.append(hidden_dim * 2 ** (i + 1))
        
        self.down_layers = nn.ModuleList()
        for i in range(num_layers):
            self.down_layers.append(nn.Sequential(
                nn.Linear(down_dims[i], down_dims[i+1]),
                nn.LayerNorm(down_dims[i+1]),
                nn.SiLU()
            ))
        
        # Middle
        self.middle = nn.Sequential(
            nn.Linear(down_dims[-1], down_dims[-1] * 2),
            nn.LayerNorm(down_dims[-1] * 2),
            nn.SiLU(),
            nn.Linear(down_dims[-1] * 2, down_dims[-1]),
            nn.LayerNorm(down_dims[-1]),
            nn.SiLU()
        )
        
        # Up path
        up_in_dims = []
        up_out_dims = []
        for i in range(num_layers):
            if i == 0:
                up_in_dims.append(down_dims[-1] + down_dims[-2])
            else:
                up_in_dims.append(down_dims[-(i+1)] + down_dims[-(i+2)])
            up_out_dims.append(down_dims[-(i+2)])
        
        self.up_layers = nn.ModuleList()
        for i in range(num_layers):
            self.up_layers.append(nn.Sequential(
                nn.Linear(up_in_dims[i], up_out_dims[i]),
                nn.LayerNorm(up_out_dims[i]),
                nn.SiLU()
            ))
        
        # Output projection
        self.output_proj = nn.Linear(hidden_dim, output_dim)
        
        # Print dimensions for debugging
        print(f"SimpleUNet dimensions:")
        print(f"  Down dims: {down_dims}")
        print(f"  Up in dims: {up_in_dims}")
        print(f"  Up out dims: {up_out_dims}")
        
    def forward(self, x, t):
        # Time embedding
        t_emb = self.time_embeddings(t)
        t_emb = self.time_mlp(t_emb)
        
        # Initial projection
        x = self.input_proj(x)
        
        # Down path with skip connections
        residuals = [x]
        for layer in self.down_layers:
            x = layer(x)
            residuals.append(x)
            
        # Middle
        x = self.middle(x)
        
        # Up path with skip connections
        for i, layer in enumerate(self.up_layers):
            residual = residuals[-(i+2)]
            x = torch.cat([x, residual], dim=-1)
            x = layer(x)
            
        # Output projection
        x = self.output_proj(x)
        
        return x


def train_energy_sampling(args):
    print("Training GFN-Diffusion with energy sampling...")
    
    # Set random seed
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    random.seed(args.seed)
    
    # Create output and model directories
    output_dir = Path(args.output_dir)
    model_dir = Path(args.model_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # Initialize wandb if specified
    if args.wandb:
        # Set offline mode if requested
        if args.offline:
            os.environ["WANDB_MODE"] = "offline"
            print("Running wandb in offline mode")
            
        try:
            import wandb
            wandb.init(
                project=args.wandb_project,
                entity=None if args.offline else args.wandb_entity,
                config=vars(args),
                name=args.run_name or f"energy_{args.energy}_gs{args.guidance_scale}"
            )
            print(f"Wandb initialized with project {args.wandb_project}")
            
            if args.offline:
                print(f"Offline run data will be saved to: {os.path.abspath('wandb')}")
        except ImportError:
            print("Wandb not found. Continuing without logging.")
            args.wandb = False
    
    # Create model and diffusion schedule
    if args.conditional:
        print("Using conditional model with multiple energy functions")
        # Conditional model
        model = ConditionalUNet(
            input_dim=args.latent_dim,
            hidden_dim=args.hidden_dim,
            num_conditions=args.num_conditions,
            output_dim=args.latent_dim,
            num_layers=args.num_layers
        ).to(args.device)
        
        # Create diffusion schedule
        diffusion = DiffusionSchedule(
            num_diffusion_timesteps=args.num_timesteps,
            schedule_type=args.schedule_type
        )
        
        # We'll create separate energy functions for each condition
        # Since we use different energy functions for different conditions,
        # we'll create them during the training loop
        
        # Visualize different energy functions
        for c in range(args.num_conditions):
            # Create a different energy function for each condition
            if c == 0:
                # Standard 25-mode GMM
                side_length = 5
                x_grid = torch.linspace(-4, 4, side_length)
                y_grid = torch.linspace(-4, 4, side_length)
                means = torch.stack(torch.meshgrid(x_grid, y_grid, indexing='ij'), dim=-1).reshape(-1, 2).to(args.device)
                weights = torch.ones(side_length * side_length).to(args.device)
                weights = weights / weights.sum()
                std = 0.1
                energy_fn = lambda x: gmm_energy(x, means, weights, std=std)
            elif c == 1:
                energy_fn = lambda x: ring_energy(x)
            elif c == 2:
                energy_fn = lambda x: moons_energy(x)
            else:
                # Modified GMM with different scale
                side_length = 5
                x_grid = torch.linspace(-4, 4, side_length)
                y_grid = torch.linspace(-4, 4, side_length)
                means = torch.stack(torch.meshgrid(x_grid, y_grid, indexing='ij'), dim=-1).reshape(-1, 2).to(args.device)
                weights = torch.ones(side_length * side_length).to(args.device)
                weights = weights / weights.sum()
                std = 0.1 * (1 + 0.5 * c)
                energy_fn = lambda x, std=std: gmm_energy(x, means, weights, std=std)
                
            visualize_energy(
                energy_fn, 
                output_dir / f"energy_cond{c}.png",
                title=f"Energy Function (Condition {c})"
            )
    else:
        # Unconditional model
        model = SimpleUNet(
            input_dim=args.latent_dim,
            hidden_dim=args.hidden_dim,
            output_dim=args.latent_dim,
            num_layers=args.num_layers
        ).to(args.device)
        
        # Create energy function
        energy_fn = setup_energy_function(args.energy, args.device)
        
        # Create diffusion schedule
        diffusion = DiffusionSchedule(
            num_diffusion_timesteps=args.num_timesteps,
            schedule_type=args.schedule_type
        )
        
        # Create GFN-Diffusion model
        gfn_diffusion = GFNDiffusion(
            model=model,
            diffusion=diffusion,
            energy_fn=energy_fn,
            guidance_scale=args.guidance_scale,
            device=args.device
        )
        
        # Visualize energy function
        visualize_energy(energy_fn, output_dir / f"energy_{args.energy}.png")
    
    # Create optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr_policy)
    
    # Training loop
    for epoch in tqdm(range(args.epochs), desc="Training"):
        # Sample batch of noise
        if args.conditional:
            # Sample conditions
            conditions = torch.zeros(args.batch_size, args.num_conditions).to(args.device)
            condition_indices = torch.randint(0, args.num_conditions, (args.batch_size,), device=args.device)
            for i in range(args.batch_size):
                conditions[i, condition_indices[i]] = 1.0
            
            # Forward diffusion
            noise = torch.randn(args.batch_size, args.latent_dim).to(args.device)
            t = torch.randint(0, args.num_timesteps, (args.batch_size,)).to(args.device)
            
            # Apply diffusion and compute loss
            alpha_cumprod = diffusion.alphas_cumprod[t].view(-1, 1)
            eps = torch.randn_like(noise)
            x_t = torch.sqrt(alpha_cumprod) * noise + torch.sqrt(1 - alpha_cumprod) * eps
            
            # Convert condition_indices to LongTensor for embedding lookup
            condition_indices = condition_indices.long()
            
            # Model prediction
            eps_pred = model(x_t, t, condition_indices)
            
            # MSE loss
            loss = F.mse_loss(eps_pred, eps)
        else:
            # Sample batch of noise
            noise = torch.randn(args.batch_size, args.latent_dim).to(args.device)
            t = torch.randint(0, args.num_timesteps, (args.batch_size,)).to(args.device)
            
            # Apply diffusion and compute loss
            alpha_cumprod = diffusion.alphas_cumprod[t].view(-1, 1)
            eps = torch.randn_like(noise)
            x_t = torch.sqrt(alpha_cumprod) * noise + torch.sqrt(1 - alpha_cumprod) * eps
            
            # Model prediction
            eps_pred = model(x_t, t)
            
            # MSE loss
            loss = F.mse_loss(eps_pred, eps)
        
        # Update model
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Logging
        if epoch % args.log_interval == 0 or epoch == args.epochs - 1:
            print(f"Epoch {epoch}: loss = {loss.item():.4f}")
            
            if args.wandb:
                wandb.log({
                    "epoch": epoch,
                    "loss": loss.item()
                })
        
        # Save model
        if (epoch > 0 and epoch % args.save_interval == 0) or epoch == args.epochs - 1:
            torch.save(model.state_dict(), model_dir / f"gfn_diffusion_epoch{epoch}.pt")
            
        # Evaluation
        if (epoch > 0 and epoch % args.eval_interval == 0) or epoch == args.epochs - 1:
            with torch.no_grad():
                model.eval()
                
                if args.conditional:
                    # Generate samples for each condition
                    for c in range(args.num_conditions):
                        # Create condition
                        condition = torch.zeros(64, args.num_conditions).to(args.device)
                        condition[:, c] = 1.0
                        
                        # Choose energy function based on condition
                        if c == 0:
                            # Standard 25-mode GMM
                            side_length = 5
                            x_grid = torch.linspace(-4, 4, side_length)
                            y_grid = torch.linspace(-4, 4, side_length)
                            means = torch.stack(torch.meshgrid(x_grid, y_grid, indexing='ij'), dim=-1).reshape(-1, 2).to(args.device)
                            weights = torch.ones(side_length * side_length).to(args.device)
                            weights = weights / weights.sum()
                            std = 0.1
                            energy_fn = lambda x: gmm_energy(x, means, weights, std=std)
                        elif c == 1:
                            energy_fn = lambda x: ring_energy(x)
                        elif c == 2:
                            energy_fn = lambda x: moons_energy(x)
                        else:
                            # Modified GMM with different scale
                            side_length = 5
                            x_grid = torch.linspace(-4, 4, side_length)
                            y_grid = torch.linspace(-4, 4, side_length)
                            means = torch.stack(torch.meshgrid(x_grid, y_grid, indexing='ij'), dim=-1).reshape(-1, 2).to(args.device)
                            weights = torch.ones(side_length * side_length).to(args.device)
                            weights = weights / weights.sum()
                            std = 0.1 * (1 + 0.5 * c)
                            energy_fn = lambda x, std=std: gmm_energy(x, means, weights, std=std)
                            
                        # Create a temporary GFNDiffusion model with this energy function
                        temp_gfn = GFNDiffusion(
                            model=model,
                            diffusion=diffusion,
                            energy_fn=energy_fn,
                            guidance_scale=args.guidance_scale,
                            device=args.device
                        )
                        
                        # Generate samples
                        with torch.no_grad():
                            # Standard diffusion (just use model directly)
                            samples_standard = []
                            x = torch.randn(64, args.latent_dim).to(args.device)
                            
                            # Create condition_indices for all samples (same condition per batch)
                            # Use the same condition for all samples in this batch to match model training
                            condition_indices = torch.full((64,), c, device=args.device, dtype=torch.long)
                            
                            # Sampling loop
                            for i in reversed(range(args.num_timesteps)):
                                t_batch = torch.full((64,), i, device=args.device, dtype=torch.long)
                                x = p_sample(diffusion, model, x, t_batch, condition_indices)
                            
                            samples_standard = x
                            
                            # GFN-guided diffusion step
                            # GFN-guided diffusion using p_sample_loop
                            samples_gfn = temp_gfn.p_sample_loop(
                                n=64, 
                                dim=args.latent_dim,
                                use_gfn=True, 
                                verbose=False,
                                condition=condition_indices
                            )
                        
                        # Visualize samples
                        visualize_samples(
                            samples_standard.cpu(), 
                            output_dir / f"samples_standard_cond{c}_epoch{epoch}.png",
                            title=f"Standard Samples (Condition {c}, Epoch {epoch})"
                        )
                        
                        visualize_samples(
                            samples_gfn.cpu(), 
                            output_dir / f"samples_gfn_cond{c}_epoch{epoch}.png",
                            title=f"GFN Samples (Condition {c}, Epoch {epoch})"
                        )
                        
                        # Compare samples
                        compare_samples(
                            samples_standard.cpu(),
                            samples_gfn.cpu(),
                            output_dir / f"compare_cond{c}_epoch{epoch}.png",
                            title=f"Sample Comparison (Condition {c}, Epoch {epoch})"
                        )
                        
                        if args.wandb:
                            wandb.log({
                                f"samples_standard_cond{c}_epoch{epoch}": 
                                    wandb.Image(str(output_dir / f"samples_standard_cond{c}_epoch{epoch}.png")),
                                f"samples_gfn_cond{c}_epoch{epoch}": 
                                    wandb.Image(str(output_dir / f"samples_gfn_cond{c}_epoch{epoch}.png")),
                                f"compare_cond{c}_epoch{epoch}": 
                                    wandb.Image(str(output_dir / f"compare_cond{c}_epoch{epoch}.png"))
                            })
                else:
                    # Generate unconditional samples
                    samples_standard = gfn_diffusion.p_sample_loop(
                        n=256,
                        use_gfn=False
                    )
                    
                    samples_gfn = gfn_diffusion.p_sample_loop(
                        n=256,
                        use_gfn=True
                    )
                    
                    # Visualize samples
                    visualize_samples(
                        samples_standard.cpu(), 
                        output_dir / f"samples_standard_epoch{epoch}.png",
                        title=f"Standard Samples (Epoch {epoch})"
                    )
                    
                    visualize_samples(
                        samples_gfn.cpu(), 
                        output_dir / f"samples_gfn_epoch{epoch}.png",
                        title=f"GFN Samples (Epoch {epoch})"
                    )
                    
                    # Compare samples
                    compare_samples(
                        samples_standard.cpu(),
                        samples_gfn.cpu(),
                        output_dir / f"compare_epoch{epoch}.png",
                        title=f"Sample Comparison (Epoch {epoch})"
                    )
                    
                    if args.wandb:
                        wandb.log({
                            f"samples_standard_epoch{epoch}": 
                                wandb.Image(str(output_dir / f"samples_standard_epoch{epoch}.png")),
                            f"samples_gfn_epoch{epoch}": 
                                wandb.Image(str(output_dir / f"samples_gfn_epoch{epoch}.png")),
                            f"compare_epoch{epoch}": 
                                wandb.Image(str(output_dir / f"compare_epoch{epoch}.png"))
                        })
                
                model.train()
    
    # Final evaluation with enhanced visualization and metrics
    print("Performing final evaluation with enhanced metrics and visualization...")
    
    # Create output directory for visualizations
    output_dir = Path(args.output_dir)
    viz_dir = output_dir / "visualizations"
    viz_dir.mkdir(exist_ok=True)
    
    # Import visualization and metrics modules
    from gfn_diffusion.utils.visualization import (
        plot_2d_density_comparison, 
        plot_3d_energy_landscape,
        plot_energy_evolution,
        visualize_diffusion_process,
        create_mode_coverage_plot,
        create_metric_comparison_plot,
        log_animated_gfn_process_to_wandb,
        create_comparative_trajectory_plot,
    )
    from gfn_diffusion.utils.metrics import (
        kl_divergence, 
        l1_distance, 
        l2_distance, 
        earth_movers_distance,
        compute_nearest_mode_distribution,
        compute_mode_coverage,
        compute_energy_statistics,
        compute_entropy,
        compute_diversity,
        compute_energy_improvement,
        compute_effective_sample_size,
        compute_coverage_metrics,
        compute_novelty,
    )
    
    # For the unconditional model, perform comprehensive evaluation
    if not args.conditional:
        # 1. Create 3D visualization of energy landscape
        plot_3d_energy_landscape(
            energy_fn=energy_fn,
            save_path=str(viz_dir / "energy_landscape_3d.png"),
            title="3D Energy Landscape"
        )
        
        # 2. Generate samples for evaluation
        with torch.no_grad():
            # Sample trajectories to visualize the diffusion process
            trajectory_standard = []
            trajectory_gfn = []
            
            # Save energy values throughout the process
            energy_values_standard = []
            energy_values_gfn = []
            
            # Create temp GFN model for sampling
            temp_gfn = GFNDiffusion(
                model=model,
                diffusion=diffusion,
                energy_fn=energy_fn,
                guidance_scale=args.guidance_scale,
                device=args.device
            )
            
            # Track samples through the reverse diffusion process
            for t in tqdm(reversed(range(0, args.num_timesteps)), desc="Sampling for visualization"):
                # Start from random noise
                if t == args.num_timesteps - 1:
                    # Create initial noise
                    x_standard = torch.randn(64, args.latent_dim).to(args.device)
                    x_gfn = x_standard.clone()  # Use same starting noise for fair comparison
                    
                    # Save initial state
                    trajectory_standard.append(x_standard.clone())
                    trajectory_gfn.append(x_gfn.clone())
                    
                    # Compute energies
                    energy_values_standard.append(energy_fn(x_standard).cpu().numpy())
                    energy_values_gfn.append(energy_fn(x_gfn).cpu().numpy())
                else:
                    # Generate sample for this timestep
                    t_tensor = torch.full((64,), t, device=args.device, dtype=torch.long)
                    
                    # Standard diffusion step
                    x_standard = p_sample(temp_gfn, model, x_standard, t_tensor)
                    trajectory_standard.append(x_standard.clone())
                    energy_values_standard.append(energy_fn(x_standard).cpu().numpy())
                    
                    # GFN-guided diffusion step
                    # Convert condition to long if needed (for embedding layers)
                    condition_long = condition.long() if hasattr(condition, 'long') else condition
                    x_gfn = temp_gfn.p_sample_with_guidance(model, x_gfn, t_tensor, condition_long)
                    trajectory_gfn.append(x_gfn.clone())
                    energy_values_gfn.append(energy_fn(x_gfn).cpu().numpy())
            
            # Convert lists to tensors/numpy arrays
            trajectory_standard = torch.stack(trajectory_standard)
            trajectory_gfn = torch.stack(trajectory_gfn)
            energy_values_standard = np.array(energy_values_standard)
            energy_values_gfn = np.array(energy_values_gfn)
            
            # Generate more samples for evaluation metrics
            samples_standard = temp_gfn.p_sample_loop(
                n=400, 
                dim=args.latent_dim,
                use_gfn=False, 
                verbose=False,
                energy_fn=energy_fn,
                guidance_scale=args.guidance_scale
            )
            
            samples_gfn = temp_gfn.p_sample_loop(
                n=400, 
                dim=args.latent_dim,
                use_gfn=True, 
                verbose=False,
                energy_fn=energy_fn,
                guidance_scale=args.guidance_scale
            )
        
        # 3. Create and log visualizations
        # Density comparison
        density_plot_path = str(viz_dir / "density_comparison.png")
        plot_2d_density_comparison(
            samples_standard=samples_standard,
            samples_gfn=samples_gfn,
            save_path=density_plot_path,
            title=f"Density Comparison (Guidance Scale={args.guidance_scale})"
        )
        
        # Energy evolution
        energy_plot_path_standard = str(viz_dir / "energy_evolution_standard.png")
        plot_energy_evolution(
            energy_values=energy_values_standard,
            save_path=energy_plot_path_standard,
            title="Energy Evolution During Standard Sampling"
        )
        
        energy_plot_path_gfn = str(viz_dir / "energy_evolution_gfn.png")
        plot_energy_evolution(
            energy_values=energy_values_gfn,
            save_path=energy_plot_path_gfn,
            title="Energy Evolution During GFN-Guided Sampling"
        )
        
        # Diffusion process visualization
        process_path_standard = str(viz_dir / "diffusion_process_standard.gif")
        visualize_diffusion_process(
            trajectory=trajectory_standard,
            save_path=process_path_standard,
            title="Standard Diffusion Process",
            energy_fn=energy_fn
        )
        
        process_path_gfn = str(viz_dir / "diffusion_process_gfn.gif")
        visualize_diffusion_process(
            trajectory=trajectory_gfn,
            save_path=process_path_gfn,
            title="GFN-Guided Diffusion Process",
            energy_fn=energy_fn
        )
        
        # Trajectory comparison at key timesteps
        timesteps = [0, 24, 49, 74, 99]  # Select key timesteps
        comparison_path = create_comparative_trajectory_plot(
            trajectory_standard=trajectory_standard,
            trajectory_gfn=trajectory_gfn,
            energy_fn=energy_fn,
            timesteps=timesteps,
            output_dir=str(viz_dir),
            name="trajectory_comparison"
        )
        
        # 4. Compute and log metrics
        # Energy statistics
        stats_standard = compute_energy_statistics(samples_standard, energy_fn)
        stats_gfn = compute_energy_statistics(samples_gfn, energy_fn)
        
        # Distribution entropy
        entropy_standard = compute_entropy(samples_standard)
        entropy_gfn = compute_entropy(samples_gfn)
        
        # Compute new metrics - diversity, energy improvement, ESS
        diversity_standard = compute_diversity(samples_standard)
        diversity_gfn = compute_diversity(samples_gfn)
        energy_improvement, top_energy_improvement = compute_energy_improvement(samples_standard, samples_gfn, energy_fn)
        standard_ess, standard_ess_ratio = compute_effective_sample_size(samples_standard, energy_fn)
        gfn_ess, gfn_ess_ratio = compute_effective_sample_size(samples_gfn, energy_fn)
        
        # Compute coverage metrics
        coverage_metrics_standard = compute_coverage_metrics(samples_standard, energy_fn=energy_fn)
        coverage_metrics_gfn = compute_coverage_metrics(samples_gfn, energy_fn=energy_fn)
        
        # If we're using a GMM energy function, compute mode coverage
        if args.energy == "25gmm":
            # Create mode centers for the 25 GMM (5x5 grid)
            xs = torch.linspace(-4, 4, 5)
            ys = torch.linspace(-4, 4, 5)
            X, Y = torch.meshgrid(xs, ys)
            modes = torch.stack([X.flatten(), Y.flatten()], dim=1)
            
            # Compute mode distribution
            mode_dist_standard = compute_nearest_mode_distribution(samples_standard, modes)
            mode_dist_gfn = compute_nearest_mode_distribution(samples_gfn, modes)
            
            # Create mode coverage plot
            mode_plot_path = str(viz_dir / "mode_coverage.png")
            create_mode_coverage_plot(
                mode_counts_standard=mode_dist_standard,
                mode_counts_gfn=mode_dist_gfn,
                save_path=mode_plot_path
            )
            
            # Compute coverage metrics
            coverage_standard, covered_modes_standard = compute_mode_coverage(samples_standard, modes, threshold=0.5)
            coverage_gfn, covered_modes_gfn = compute_mode_coverage(samples_gfn, modes, threshold=0.5)
            
            # Compute novelty metrics with respect to modes
            novelty_standard = compute_novelty(samples_standard, modes)
            novelty_gfn = compute_novelty(samples_gfn, modes)
            
            # Earth mover's distance between mode distributions
            emd = earth_movers_distance(mode_dist_standard, mode_dist_gfn)
            
            # Log mode coverage metrics
            if args.wandb:
                wandb.log({
                    "final_mode_coverage/standard": coverage_standard,
                    "final_mode_coverage/gfn": coverage_gfn,
                    "final_mode_coverage/ratio": coverage_gfn / (coverage_standard + 1e-8),
                    "final_mode_coverage/emd": emd,
                    "final_mode_coverage/novelty_standard": novelty_standard,
                    "final_mode_coverage/novelty_gfn": novelty_gfn,
                    "final_mode_coverage_plot": wandb.Image(mode_plot_path)
                })
        
        # Compile all metrics
        metrics = {
            "Standard Diffusion": {
                "mean_energy": stats_standard["mean_energy"],
                "min_energy": stats_standard["min_energy"],
                "entropy": entropy_standard,
                "diversity": diversity_standard,
                "effective_sample_size": standard_ess_ratio
            },
            "GFN-Guided Diffusion": {
                "mean_energy": stats_gfn["mean_energy"],
                "min_energy": stats_gfn["min_energy"],
                "entropy": entropy_gfn,
                "diversity": diversity_gfn,
                "effective_sample_size": gfn_ess_ratio
            }
        }
        
        # Add grid coverage to metrics
        if 'grid_coverage' in coverage_metrics_standard:
            metrics["Standard Diffusion"]["grid_coverage"] = coverage_metrics_standard["grid_coverage"]
            metrics["GFN-Guided Diffusion"]["grid_coverage"] = coverage_metrics_gfn["grid_coverage"]
            
        # Additional metrics for GMM energy
        if args.energy == "25gmm":
            metrics["Standard Diffusion"]["mode_coverage"] = coverage_standard
            metrics["GFN-Guided Diffusion"]["mode_coverage"] = coverage_gfn
            metrics["Standard Diffusion"]["novelty"] = novelty_standard
            metrics["GFN-Guided Diffusion"]["novelty"] = novelty_gfn
        
        # Create metric comparison plot
        metric_plot_path = str(viz_dir / "metric_comparison.png")
        create_metric_comparison_plot(
            metrics_dict=metrics,
            save_path=metric_plot_path,
            title="Performance Metrics Comparison",
            higher_is_better=False  # Lower energy is better
        )
        
        # 5. Log everything to wandb
        if args.wandb:
            wandb.log({
                # Images
                "viz/density_comparison": wandb.Image(density_plot_path),
                "viz/energy_evolution_standard": wandb.Image(energy_plot_path_standard),
                "viz/energy_evolution_gfn": wandb.Image(energy_plot_path_gfn),
                "viz/diffusion_process_standard": wandb.Image(process_path_standard),
                "viz/diffusion_process_gfn": wandb.Image(process_path_gfn),
                "viz/trajectory_comparison": wandb.Image(comparison_path),
                "viz/metric_comparison": wandb.Image(metric_plot_path),
                
                # Energy statistics
                "energy_stats/standard/mean": stats_standard["mean_energy"],
                "energy_stats/standard/min": stats_standard["min_energy"],
                "energy_stats/standard/max": stats_standard["max_energy"],
                "energy_stats/standard/std": stats_standard["std_energy"],
                "energy_stats/standard/p50": stats_standard["p50"],
                
                "energy_stats/gfn/mean": stats_gfn["mean_energy"],
                "energy_stats/gfn/min": stats_gfn["min_energy"],
                "energy_stats/gfn/max": stats_gfn["max_energy"],
                "energy_stats/gfn/std": stats_gfn["std_energy"],
                "energy_stats/gfn/p50": stats_gfn["p50"],
                
                # Energy improvement metrics
                "energy_improvement/mean": energy_improvement,
                "energy_improvement/top_10_percent": top_energy_improvement,
                
                # Distribution metrics
                "distribution/entropy_standard": entropy_standard,
                "distribution/entropy_gfn": entropy_gfn,
                "distribution/diversity_standard": diversity_standard,
                "distribution/diversity_gfn": diversity_gfn,
                "distribution/diversity_ratio": diversity_gfn / (diversity_standard + 1e-8),
                
                # Effective sample size metrics
                "ess/standard": standard_ess,
                "ess/standard_ratio": standard_ess_ratio,
                "ess/gfn": gfn_ess,
                "ess/gfn_ratio": gfn_ess_ratio,
                "ess/improvement": (gfn_ess_ratio - standard_ess_ratio) / (standard_ess_ratio + 1e-8) * 100,
                
                # Coverage metrics
                "coverage/standard_grid": coverage_metrics_standard.get("grid_coverage", 0),
                "coverage/gfn_grid": coverage_metrics_gfn.get("grid_coverage", 0),
                "coverage/grid_ratio": coverage_metrics_gfn.get("grid_coverage", 0) / (coverage_metrics_standard.get("grid_coverage", 1e-8)),
            })
            
            # Log animated GIF for GFN process
            log_animated_gfn_process_to_wandb(
                trajectories=[trajectory_gfn],
                energy_fn=energy_fn,
                output_dir=str(viz_dir),
                name="gfn_sampling_process"
            )
    
    # For conditional models, perform evaluation for each condition
    else:
        for c in range(args.num_conditions):
            print(f"Evaluating for condition {c}...")
            
            # 1. Get the energy function for this condition
            if c == 0:
                # Use 25gmm for condition 0
                xs = torch.linspace(-4, 4, 5)
                ys = torch.linspace(-4, 4, 5)
                X, Y = torch.meshgrid(xs, ys)
                means = torch.stack([X.flatten(), Y.flatten()], dim=1).to(args.device)
                weights = torch.ones(25).to(args.device) / 25
                std = 0.1
                cond_energy_fn = lambda x: gmm_energy(x, means, weights, std)
            elif c == 1:
                # Use ring energy for condition 1
                cond_energy_fn = ring_energy
            elif c == 2:
                # Use moons energy for condition 2
                cond_energy_fn = moons_energy
            else:
                # Use modified GMM for other conditions
                xs = torch.linspace(-4, 4, 5)
                ys = torch.linspace(-4, 4, 5)
                X, Y = torch.meshgrid(xs, ys)
                means = torch.stack([X.flatten(), Y.flatten()], dim=1).to(args.device)
                weights = torch.ones(25).to(args.device) / 25
                std = 0.1 * (1 + c * 0.5)  # Increase std with condition
                cond_energy_fn = lambda x: gmm_energy(x, means, weights, std)
            
            # 2. Create 3D visualization of energy landscape
            plot_3d_energy_landscape(
                energy_fn=cond_energy_fn,
                save_path=str(viz_dir / f"energy_landscape_3d_cond{c}.png"),
                title=f"3D Energy Landscape (Condition {c})"
            )
            
            # 3. Generate samples for this condition
            with torch.no_grad():
                # Sample trajectories to visualize the diffusion process
                trajectory_standard = []
                trajectory_gfn = []
                
                # Save energy values throughout the process
                energy_values_standard = []
                energy_values_gfn = []
                
                # Create temp GFN model for sampling
                temp_gfn = GFNDiffusion(
                    model=model,
                    diffusion=diffusion,
                    energy_fn=cond_energy_fn,
                    guidance_scale=args.guidance_scale,
                    device=args.device
                )
                
                # Create condition tensor
                condition = torch.full((64,), c, device=args.device, dtype=torch.long)
                
                # Track samples through the reverse diffusion process
                for t in tqdm(reversed(range(0, args.num_timesteps)), desc=f"Sampling for condition {c}"):
                    # Start from random noise
                    if t == args.num_timesteps - 1:
                        # Create initial noise
                        x_standard = torch.randn(64, args.latent_dim).to(args.device)
                        x_gfn = x_standard.clone()  # Use same starting noise for fair comparison
                        
                        # Save initial state
                        trajectory_standard.append(x_standard.clone())
                        trajectory_gfn.append(x_gfn.clone())
                        
                        # Compute energies
                        energy_values_standard.append(cond_energy_fn(x_standard).cpu().numpy())
                        energy_values_gfn.append(cond_energy_fn(x_gfn).cpu().numpy())
                    else:
                        # Generate sample for this timestep
                        t_tensor = torch.full((64,), t, device=args.device, dtype=torch.long)
                        
                        # Standard diffusion step
                        x_standard = p_sample(temp_gfn, model, x_standard, t_tensor, condition)
                        trajectory_standard.append(x_standard.clone())
                        energy_values_standard.append(cond_energy_fn(x_standard).cpu().numpy())
                        
                        # GFN-guided diffusion step
                        # Convert condition to long if needed (for embedding layers)
                        condition_long = condition.long() if hasattr(condition, 'long') else condition
                        x_gfn = temp_gfn.p_sample_with_guidance(model, x_gfn, t_tensor, condition_long)
                        trajectory_gfn.append(x_gfn.clone())
                        energy_values_gfn.append(cond_energy_fn(x_gfn).cpu().numpy())
                
                # Convert lists to tensors/numpy arrays
                trajectory_standard = torch.stack(trajectory_standard)
                trajectory_gfn = torch.stack(trajectory_gfn)
                energy_values_standard = np.array(energy_values_standard)
                energy_values_gfn = np.array(energy_values_gfn)
                
                # Generate more samples for evaluation metrics
                samples_standard = temp_gfn.p_sample_loop(
                    n=400, 
                    dim=args.latent_dim,
                    use_gfn=False, 
                    verbose=False,
                    condition=torch.full((400,), c, dtype=torch.long, device=args.device)
                )
                
                samples_gfn = temp_gfn.p_sample_loop(
                    n=400, 
                    dim=args.latent_dim,
                    use_gfn=True, 
                    verbose=False,
                    condition=torch.full((400,), c, dtype=torch.long, device=args.device)
                )
            
            # 4. Create and log visualizations
            # Density comparison
            density_plot_path = str(viz_dir / f"density_comparison_cond{c}.png")
            plot_2d_density_comparison(
                samples_standard=samples_standard,
                samples_gfn=samples_gfn,
                save_path=density_plot_path,
                title=f"Density Comparison (Condition {c}, Guidance Scale={args.guidance_scale})"
            )
            
            # Energy evolution
            energy_plot_path_standard = str(viz_dir / f"energy_evolution_standard_cond{c}.png")
            plot_energy_evolution(
                energy_values=energy_values_standard,
                save_path=energy_plot_path_standard,
                title=f"Energy Evolution During Standard Sampling (Condition {c})"
            )
            
            energy_plot_path_gfn = str(viz_dir / f"energy_evolution_gfn_cond{c}.png")
            plot_energy_evolution(
                energy_values=energy_values_gfn,
                save_path=energy_plot_path_gfn,
                title=f"Energy Evolution During GFN-Guided Sampling (Condition {c})"
            )
            
            # Diffusion process visualization
            process_path_standard = str(viz_dir / f"diffusion_process_standard_cond{c}.gif")
            visualize_diffusion_process(
                trajectory=trajectory_standard,
                save_path=process_path_standard,
                title=f"Standard Diffusion Process (Condition {c})",
                energy_fn=cond_energy_fn
            )
            
            process_path_gfn = str(viz_dir / f"diffusion_process_gfn_cond{c}.gif")
            visualize_diffusion_process(
                trajectory=trajectory_gfn,
                save_path=process_path_gfn,
                title=f"GFN-Guided Diffusion Process (Condition {c})",
                energy_fn=cond_energy_fn
            )
            
            # Trajectory comparison at key timesteps
            timesteps = [0, 24, 49, 74, 99]  # Select key timesteps
            comparison_path = create_comparative_trajectory_plot(
                trajectory_standard=trajectory_standard,
                trajectory_gfn=trajectory_gfn,
                energy_fn=cond_energy_fn,
                timesteps=timesteps,
                output_dir=str(viz_dir),
                name=f"trajectory_comparison_cond{c}"
            )
            
            # 5. Compute and log metrics
            # Energy statistics
            stats_standard = compute_energy_statistics(samples_standard, cond_energy_fn)
            stats_gfn = compute_energy_statistics(samples_gfn, cond_energy_fn)
            
            # Distribution entropy
            entropy_standard = compute_entropy(samples_standard)
            entropy_gfn = compute_entropy(samples_gfn)
            
            # Compute new metrics - diversity, energy improvement, ESS
            diversity_standard = compute_diversity(samples_standard)
            diversity_gfn = compute_diversity(samples_gfn)
            energy_improvement, top_energy_improvement = compute_energy_improvement(samples_standard, samples_gfn, cond_energy_fn)
            standard_ess, standard_ess_ratio = compute_effective_sample_size(samples_standard, cond_energy_fn)
            gfn_ess, gfn_ess_ratio = compute_effective_sample_size(samples_gfn, cond_energy_fn)
            
            # Compute coverage metrics
            coverage_metrics_standard = compute_coverage_metrics(samples_standard, energy_fn=cond_energy_fn)
            coverage_metrics_gfn = compute_coverage_metrics(samples_gfn, energy_fn=cond_energy_fn)
            
            # If we're using a GMM energy function for this condition, compute mode coverage
            if c == 0:  # 25gmm
                # Create mode centers for the 25 GMM (5x5 grid)
                xs = torch.linspace(-4, 4, 5)
                ys = torch.linspace(-4, 4, 5)
                X, Y = torch.meshgrid(xs, ys)
                modes = torch.stack([X.flatten(), Y.flatten()], dim=1)
                
                # Compute mode distribution
                mode_dist_standard = compute_nearest_mode_distribution(samples_standard, modes)
                mode_dist_gfn = compute_nearest_mode_distribution(samples_gfn, modes)
                
                # Create mode coverage plot
                mode_plot_path = str(viz_dir / f"mode_coverage_cond{c}.png")
                create_mode_coverage_plot(
                    mode_counts_standard=mode_dist_standard,
                    mode_counts_gfn=mode_dist_gfn,
                    save_path=mode_plot_path,
                    title=f"Mode Coverage Comparison (Condition {c})"
                )
                
                # Compute coverage metrics
                coverage_standard, covered_modes_standard = compute_mode_coverage(samples_standard, modes, threshold=0.5)
                coverage_gfn, covered_modes_gfn = compute_mode_coverage(samples_gfn, modes, threshold=0.5)
                
                # Compute novelty metrics with respect to modes
                novelty_standard = compute_novelty(samples_standard, modes)
                novelty_gfn = compute_novelty(samples_gfn, modes)
                
                # Earth mover's distance between mode distributions
                emd = earth_movers_distance(mode_dist_standard, mode_dist_gfn)
                
                # Log mode coverage metrics
                if args.wandb:
                    wandb.log({
                        f"final_mode_coverage_cond{c}/standard": coverage_standard,
                        f"final_mode_coverage_cond{c}/gfn": coverage_gfn,
                        f"final_mode_coverage_cond{c}/ratio": coverage_gfn / (coverage_standard + 1e-8),
                        f"final_mode_coverage_cond{c}/emd": emd,
                        f"final_mode_coverage_cond{c}/novelty_standard": novelty_standard,
                        f"final_mode_coverage_cond{c}/novelty_gfn": novelty_gfn,
                        f"final_mode_coverage_plot_cond{c}": wandb.Image(mode_plot_path)
                    })
            
            # Compile all metrics
            metrics = {
                "Standard Diffusion": {
                    "mean_energy": stats_standard["mean_energy"],
                    "min_energy": stats_standard["min_energy"],
                    "entropy": entropy_standard,
                    "diversity": diversity_standard,
                    "effective_sample_size": standard_ess_ratio
                },
                "GFN-Guided Diffusion": {
                    "mean_energy": stats_gfn["mean_energy"],
                    "min_energy": stats_gfn["min_energy"],
                    "entropy": entropy_gfn,
                    "diversity": diversity_gfn,
                    "effective_sample_size": gfn_ess_ratio
                }
            }
            
            # Add grid coverage to metrics
            if 'grid_coverage' in coverage_metrics_standard:
                metrics["Standard Diffusion"]["grid_coverage"] = coverage_metrics_standard["grid_coverage"]
                metrics["GFN-Guided Diffusion"]["grid_coverage"] = coverage_metrics_gfn["grid_coverage"]
            
            # Additional metrics for GMM energy in condition 0
            if c == 0:
                metrics["Standard Diffusion"]["mode_coverage"] = coverage_standard
                metrics["GFN-Guided Diffusion"]["mode_coverage"] = coverage_gfn
                metrics["Standard Diffusion"]["novelty"] = novelty_standard
                metrics["GFN-Guided Diffusion"]["novelty"] = novelty_gfn
            
            # Create metric comparison plot
            metric_plot_path = str(viz_dir / f"metric_comparison_cond{c}.png")
            create_metric_comparison_plot(
                metrics_dict=metrics,
                save_path=metric_plot_path,
                title=f"Performance Metrics Comparison (Condition {c})",
                higher_is_better=False  # Lower energy is better
            )
            
            # 6. Log everything to wandb
            if args.wandb:
                wandb.log({
                    # Images
                    f"viz/density_comparison_cond{c}": wandb.Image(density_plot_path),
                    f"viz/energy_evolution_standard_cond{c}": wandb.Image(energy_plot_path_standard),
                    f"viz/energy_evolution_gfn_cond{c}": wandb.Image(energy_plot_path_gfn),
                    f"viz/diffusion_process_standard_cond{c}": wandb.Image(process_path_standard),
                    f"viz/diffusion_process_gfn_cond{c}": wandb.Image(process_path_gfn),
                    f"viz/trajectory_comparison_cond{c}": wandb.Image(comparison_path),
                    f"viz/metric_comparison_cond{c}": wandb.Image(metric_plot_path),
                    
                    # Energy statistics
                    f"energy_stats_cond{c}/standard/mean": stats_standard["mean_energy"],
                    f"energy_stats_cond{c}/standard/min": stats_standard["min_energy"],
                    f"energy_stats_cond{c}/standard/max": stats_standard["max_energy"],
                    f"energy_stats_cond{c}/standard/std": stats_standard["std_energy"],
                    f"energy_stats_cond{c}/standard/p50": stats_standard["p50"],
                    
                    f"energy_stats_cond{c}/gfn/mean": stats_gfn["mean_energy"],
                    f"energy_stats_cond{c}/gfn/min": stats_gfn["min_energy"],
                    f"energy_stats_cond{c}/gfn/max": stats_gfn["max_energy"],
                    f"energy_stats_cond{c}/gfn/std": stats_gfn["std_energy"],
                    f"energy_stats_cond{c}/gfn/p50": stats_gfn["p50"],
                    
                    # Energy improvement metrics
                    f"energy_improvement_cond{c}/mean": energy_improvement,
                    f"energy_improvement_cond{c}/top_10_percent": top_energy_improvement,
                    
                    # Distribution metrics
                    f"distribution_cond{c}/entropy_standard": entropy_standard,
                    f"distribution_cond{c}/entropy_gfn": entropy_gfn,
                    f"distribution_cond{c}/diversity_standard": diversity_standard,
                    f"distribution_cond{c}/diversity_gfn": diversity_gfn,
                    f"distribution_cond{c}/diversity_ratio": diversity_gfn / (diversity_standard + 1e-8),
                    
                    # Effective sample size metrics
                    f"ess_cond{c}/standard": standard_ess,
                    f"ess_cond{c}/standard_ratio": standard_ess_ratio,
                    f"ess_cond{c}/gfn": gfn_ess,
                    f"ess_cond{c}/gfn_ratio": gfn_ess_ratio,
                    f"ess_cond{c}/improvement": (gfn_ess_ratio - standard_ess_ratio) / (standard_ess_ratio + 1e-8) * 100,
                    
                    # Coverage metrics
                    f"coverage_cond{c}/standard_grid": coverage_metrics_standard.get("grid_coverage", 0),
                    f"coverage_cond{c}/gfn_grid": coverage_metrics_gfn.get("grid_coverage", 0),
                    f"coverage_cond{c}/grid_ratio": coverage_metrics_gfn.get("grid_coverage", 0) / (coverage_metrics_standard.get("grid_coverage", 1e-8)),
                })
                
                # Log animated GIF for GFN process
                log_animated_gfn_process_to_wandb(
                    trajectories=[trajectory_gfn],
                    energy_fn=cond_energy_fn,
                    output_dir=str(viz_dir),
                    name=f"gfn_sampling_process_cond{c}"
                )
                
    # Return the trained model
    return model


def main():
    """
    Main function to parse arguments and run training.
    """
    args = parse_args()
    
    # Set random seed for reproducibility
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    # Create results directory
    os.makedirs("results", exist_ok=True)
    
    # Run appropriate training function
    model = train_energy_sampling(args)
    
    print("Training completed successfully!")


if __name__ == "__main__":
    main() 